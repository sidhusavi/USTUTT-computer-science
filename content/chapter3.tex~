%Die Angabe des schlauen Spruchs auf diesem Wege funtioniert nur,
%wenn keine Änderung des Kapitels mittels den in preambel/chapterheads.tex
%vorgeschlagenen Möglichkeiten durchgeführt wurde.
\chapter{Machine Learning}
\label{chap:chapter3}
%\vspace{-3cm}
%\vspace{2cm}
An \emph{algorithm} is set of instructions used to convert input values to output, based on certain rules. Consider an example where we need to find all even numbers from dataset. Here, we can set up a \emph{rule} that if number is completely divisible by two then it should be included in the output dataset, otherwise not. Naturally, as there can be more than one way to solve a problem, there can be more than one algorithm to solve it. However there are certain examples where formation of set of rule is practically infeasible. For example, consider handwriting recognition software used to scan handwritten forms. Figure illustrates problem at hand, where a simple character can be written in a number of ways. It is interesting to note that humans are able to read this data without trouble, but it is really difficult express a certain rules which will result in accurate recognition with help of an algorithm. Machine learning is employed in such cases. Specifically \emph{Machine Learning} (ML) is programming computers to optimize a performance criterion (e.g. character recognition) using example data or past experience \cite{Alpaydin2004}. 

\begin{figure}[h]
  \begin{center}
    \captionsetup{justification=centering}
    \includegraphics[scale=0.5]{figures/charrec.png}
    \caption{Example of Machine Learning: Character recognition}
    \label{fig:charrec}
  \end{center}
\end{figure}

The "example data" with its \emph{label} is collectively called as \emph{training set}, and it is used to teach machine learning how the character with given label looks like, so that ML can recognize when it encounters similar data in future. Machine learning can be applied in wide range of applications where it is not possible to express expertise but a large amount of sample data is available. Typical applications of machine learning include computer vision, pattern recognition, spam filtering, search result optimization etc. 

\section{Basics of Machine Learning}
\label{sec:c3basics}

\subsection{Types of learning algorithms}
Based on application, ML algorithms can be can be classified in two major categories \emph{viz.} supervised learning and unsupervised learning. 

\emph{Supervised learning} algorithms are used when labels of the data to be are known. A spam filter is a good example where supervised learning can be used for \emph{classification}. Here we know an email received is either "spam" or "not-spam", these categories can be used as labels for the sample population and learning algorithm can classify within these two type.  One more application of supervised learning is to predict a numerical value in \emph{regression}. Consider a problem to predict value of a used property, the input parameters in this case are initial value, year of construction, size of property, locality and so on, whereas output is current resale value. one can construct a training set of known resale values and receptive values of input parameters and train leaning algorithm to predict other inputs. To generalize, aim in supervised learning is to learn mapping from input to output whose correct vales are provided by supervisor \cite{Alpaydin2004}.

\emph{Unsupervised learning} or \emph{clustering} is used in classification problems where the labels for the data are not known. An example of such problem is document clustering \cite{Alpaydin2004}. One of applications of document clustering is to cluster news reports which belong to same category like sport, science, art and so on. The number of such categories is not clear, and the machine learning application in such case needs to cluster articles based on some common words, and provide the supervisor data, which he may use to label clustered groups.

In case of fault classification, we have clearly defined taxonomy in earlier chapter, making our case as supervised classification problem. In following subsection, we define basic terms as applied to case of supervised learning.

\subsection{Features and Feature Selection}
A \emph{feature} $(x_i)$ is a result of measurement made on a unit input data. Generally, a set of features $(\boldsymbol{x}^t)$ is needed to characterize a unit of input data and is expressed as,
\[ \boldsymbol{x}^t = \left[ x_1, x_2, \ldots x_m \right]^T \]  
Its label $r$ denotes the class $C_i \in \{C_1, C_2 \ldots C_k\}$ it belongs to and is denoted as,
\[ r_i^t = \left\{ \begin{array}{ll}
         1 & \mbox{if $\boldsymbol{x}^t \in C_i$};\\
         0 & \mbox{if $\boldsymbol{x}^t \in C_j  and  j \neq i$}\end{array} \right. \] 
The training set $X$ is then defined as ordered set containing $N$ values of such examples,
\[ X = \{\boldsymbol{x}^t , \boldsymbol{r}^t \}_{t=1}^N  \]
The aim for machine learning algorithm is to learn values in training set and then classify new examples $\boldsymbol{x}$ by estimating value of $C(\boldsymbol{x})$. To achieve this, the algorithm tries to find out a hypotheses $h_i, i \in\{1,2, \ldots k\}$ from a set of all possible hypotheses such that,
\[ h_i(\boldsymbol{x}) = \left\{ \begin{array}{ll}
         1 & \mbox{if $\boldsymbol{x}^t \in C_i$};\\
         0 & \mbox{$\boldsymbol{x}^t \in C_j  and  j \neq i$}\end{array} \right. \] 
The \emph{empirical error} after training is calculated as,
\[ E(\{h\}_{i=1}^k|X) = \sum\limits_{t=1}^N \sum\limits_{i=1}^k | h_i(\boldsymbol{x}) \neq r_i^t ) | \]

Figure~\ref{fig:mlfitting} shows two possible hypotheses $h_1$ and $h_2$ for a simple 2-class classification problem, both with same value of empirical error and also actual boundary of classification $C$. If we choose hypothesis $h_1$ then the examples which lie in region between $h_1$ and $C$ will get incorrectly classified and this is called as \emph{overfitting}. On the other hand, if we choose $h_2$ then same will happen for examples in region between $C$ and $h_2$, called \emph{underfitting}. To avoid this and to get a hypothesis which is as close to $C$ as possible, one more labeled dataset with examples other than training set called as \emph{cross-validation set} is picked. The empirical error is then calculated over this set and hypotheses obtained during training, and the hypothesis with least value of error is selected. 

\begin{figure}[h]
  \begin{center}
    \captionsetup{justification=centering}
    \includegraphics[scale=0.45]{figures/mlfitting.png}
    \caption{Example of overfitting and underfitting}
    \label{fig:mlfitting}
  \end{center}
\end{figure}

We use the term \emph{sample population} collectively for training set and cross-validation set.

\subsection{Sample population}
Hi hrello bol do


\section{Machine Learning Algorithms for Classification}
\label{sec:c3mlclassification}

\subsection{Bayesian networks}

\subsection{Decision Trees}

\subsection{Multilayered Perceptrons}

\subsection{Support Vector Machines}

\section{Criterion To Choose Suitable ML Algorithm}
\label{sec:c3mlselection}
